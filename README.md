#                  Projet d‚ÄôIntelligence Artificielle : Deep Learning 

Ce d√©p√¥t contient deux projets explorant l‚Äôutilisation des r√©seaux de neurones pour des t√¢ches diff√©rentes :  

1. **Projet_1 : Classification d‚Äôimages de routes et champs avec CNN**  
2. **Projet_2 : Extraction et classification de d√©finitions textuelles avec MLP, CNN et LSTM**  

---

# üåæ Projet_1: Classification de routes et champs par CNN

## Description
Ce projet explore la **classification d‚Äôimages** en deux cat√©gories : **routes** et **champs**, en comparant trois types de repr√©sentations :  

- **RGB** : images couleur originales  
- **Niveaux de gris (L)** : conversion en nuances de gris  
- **Niveaux de gris + √©galisation locale (L-LHE)** : pr√©-traitement pour am√©liorer le contraste  

**Objectif** : d√©terminer si la couleur est n√©cessaire ou si des pr√©-traitements am√©liorent la performance d‚Äôun CNN.

---

## Jeu de donn√©es
- **Nombre total d‚Äôimages** : 90  
- **Cat√©gories** : `route`, `champ`  
- **Taille des images** : 150 √ó 150 pixels  

| Type de donn√©es | Description |
|-----------------|------------|
| RGB             | Image couleur originale |
| L               | Conversion en niveaux de gris |
| L-LHE           | Niveaux de gris + √©galisation locale d‚Äôhistogramme |

---

## Architecture du mod√®le CNN
- **2 blocs convolution + max pooling**  
- **Couche Flatten** pour aplatir les feature maps  
- **Couche Dense** : 128 neurones, activation ReLU  
- **Couche de sortie** : 1 neurone, activation sigmo√Øde  
- **Dropout** : 0.5 (r√©gularisation)  
- **Optimiseur** : Adam  
- **Fonction de perte** : binary_crossentropy  

---

## R√©sultats

| Jeu de donn√©es | Pr√©cision entra√Ænement | Pr√©cision validation |
|----------------|------------------------|---------------------|
| RGB            | ~87,5 %               | ~66,7 %             |
| L (gris)       | ~90 %                 | ~83,3 %             |
| L-LHE          | ~75 % (instable)      | ~50‚Äì55 %            |

### Interpr√©tation
- ‚úÖ **Niveaux de gris (L)** : meilleur compromis apprentissage / g√©n√©ralisation  
- ‚ö†Ô∏è **RGB** : risque de surapprentissage  
- ‚ùå **L-LHE** : √©galisation locale trop perturbante pour le CNN  

---

## Am√©liorations possibles
- Augmentation de donn√©es (rotations, flips, variations de luminosit√©)  
- R√©gularisation des couches convolutionnelles (`kernel_regularizer=L2`)  
- Ajustement de la taille du batch (8 ou 16)  
- Early stopping pour limiter le surapprentissage  
- Validation crois√©e k-fold pour plus de robustesse  
- Apprentissage par transfert avec mod√®les pr√©-entra√Æn√©s (VGG16, MobileNetV2, ResNet)  

---

## Conclusion
- La **couleur n‚Äôest pas indispensable**  
- L‚Äô**√©galisation locale** peut d√©grader les performances  
- Avec un petit dataset, **augmentation de donn√©es** et/ou **apprentissage par transfert** sont fortement recommand√©s  

---

# üöÄ Projet_2: Extraction et classification de d√©finitions

## Objectif
Classifier automatiquement des d√©finitions en **bonnes** ou **mauvaises**, √† l‚Äôaide de **r√©seaux de neurones** (MLP, CNN, LSTM).

---

## Pr√©traitement des donn√©es
- Fichiers : `wiki_good.txt` et `wiki_bad.txt`  
- Cr√©ation d‚Äôun DataFrame :  
  - `CONTENT` : texte de la d√©finition  
  - `CLASS` : √©tiquette (`1` = bonne, `0` = mauvaise)  
- Nettoyage du texte :  
  - Suppression parenth√®ses, chiffres, ponctuation  
  - Conversion en minuscules, tokenisation  
  - Suppression des stopwords (sauf *is, type, an*)  
  - Lemmatisation  

---

## Construction des jeux de donn√©es
- S√©paration : 80% **train**, 20% **test**  
- Tokenisation : 5000 mots les plus fr√©quents  
- S√©quences normalis√©es (`pad_sequences`, longueur 50)  
- Embedding pr√©-entra√Æn√© **GloVe (50 dimensions)**  

---

## Mod√®les test√©s

| Mod√®le | Accuracy test | Commentaire |
|--------|---------------|------------|
| MLP    | ~83%          | Overfitting l√©ger (√©cart train/val ‚âà 0.08) |
| CNN 1D | ~87%          | Performance meilleure mais overfitting plus marqu√© (√©cart ‚âà 0.10) |
| LSTM   | ~89%          | Meilleur mod√®le, √©cart moyen plus faible (~0.06), adapt√© aux s√©quences |

---

## Optimisation des hyperparam√®tres (LSTM)
- Meilleure configuration : **128 neurones, batch_size = 200**  
- Accuracy sur test set : **‚âà 90%**

---

## Pr√©diction sur une instance
- Exemple r√©el ‚Üí pr√©diction correcte (sigmoid > 0.5)  

---

## R√©sultats
- Le **LSTM** est le mod√®le le plus performant  
- Bonne g√©n√©ralisation avec **‚âà 90% d‚Äôaccuracy** sur le test set  

---

## Technologies utilis√©es
- **Python**  
- **NLTK, Pandas, Numpy, Scikit-learn**  
- **Keras (Tensorflow backend)**  
- **Matplotlib, Seaborn**  
- **GloVe embeddings**  

---

## Conclusion
- Les **LSTM** surpassent les mod√®les MLP et CNN  
- Leur capacit√© √† capturer les **d√©pendances s√©quentielles du langage** explique la performance  

---
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

